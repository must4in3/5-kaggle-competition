{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_train = pd.read_csv('outputs/df_train.csv')\n",
    "diamonds_train_price = pd.read_csv('outputs/df_train_price.csv')\n",
    "diamonds_predict=pd.read_csv('outputs/df_test.csv')\n",
    "submit=pd.read_csv('inputs/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut_numeric</th>\n",
       "      <th>color_numeric</th>\n",
       "      <th>clarity_numeric</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.051355</td>\n",
       "      <td>-0.810564</td>\n",
       "      <td>0.352161</td>\n",
       "      <td>-0.033632</td>\n",
       "      <td>0.736126</td>\n",
       "      <td>-0.650176</td>\n",
       "      <td>-1.283716</td>\n",
       "      <td>-1.236438</td>\n",
       "      <td>-1.180531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.966764</td>\n",
       "      <td>0.982588</td>\n",
       "      <td>0.940769</td>\n",
       "      <td>-0.641579</td>\n",
       "      <td>0.596363</td>\n",
       "      <td>-1.097476</td>\n",
       "      <td>-1.132102</td>\n",
       "      <td>-1.080061</td>\n",
       "      <td>-1.039596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.839879</td>\n",
       "      <td>-0.810564</td>\n",
       "      <td>1.529377</td>\n",
       "      <td>-0.641579</td>\n",
       "      <td>-1.010919</td>\n",
       "      <td>2.033618</td>\n",
       "      <td>-0.918059</td>\n",
       "      <td>-0.854183</td>\n",
       "      <td>-0.969129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.839879</td>\n",
       "      <td>0.086012</td>\n",
       "      <td>-0.825055</td>\n",
       "      <td>0.574315</td>\n",
       "      <td>0.037308</td>\n",
       "      <td>0.781181</td>\n",
       "      <td>-0.900222</td>\n",
       "      <td>-0.862870</td>\n",
       "      <td>-0.870475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.217504</td>\n",
       "      <td>-0.810564</td>\n",
       "      <td>1.529377</td>\n",
       "      <td>-0.641579</td>\n",
       "      <td>-0.521746</td>\n",
       "      <td>2.480917</td>\n",
       "      <td>0.330526</td>\n",
       "      <td>0.344710</td>\n",
       "      <td>0.271096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      carat  cut_numeric  color_numeric  clarity_numeric     depth     table  \\\n",
       "0 -1.051355    -0.810564       0.352161        -0.033632  0.736126 -0.650176   \n",
       "1 -0.966764     0.982588       0.940769        -0.641579  0.596363 -1.097476   \n",
       "2 -0.839879    -0.810564       1.529377        -0.641579 -1.010919  2.033618   \n",
       "3 -0.839879     0.086012      -0.825055         0.574315  0.037308  0.781181   \n",
       "4  0.217504    -0.810564       1.529377        -0.641579 -0.521746  2.480917   \n",
       "\n",
       "          x         y         z  \n",
       "0 -1.283716 -1.236438 -1.180531  \n",
       "1 -1.132102 -1.080061 -1.039596  \n",
       "2 -0.918059 -0.854183 -0.969129  \n",
       "3 -0.900222 -0.862870 -0.870475  \n",
       "4  0.330526  0.344710  0.271096  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamonds_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut_numeric</th>\n",
       "      <th>color_numeric</th>\n",
       "      <th>clarity_numeric</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.267015</td>\n",
       "      <td>0.978152</td>\n",
       "      <td>-1.416151</td>\n",
       "      <td>-1.232414</td>\n",
       "      <td>-0.666318</td>\n",
       "      <td>-1.554540</td>\n",
       "      <td>2.620734</td>\n",
       "      <td>2.539800</td>\n",
       "      <td>2.446736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.596725</td>\n",
       "      <td>0.978152</td>\n",
       "      <td>-0.830504</td>\n",
       "      <td>-1.232414</td>\n",
       "      <td>0.168508</td>\n",
       "      <td>-0.658049</td>\n",
       "      <td>2.175567</td>\n",
       "      <td>2.234832</td>\n",
       "      <td>2.230593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.608098</td>\n",
       "      <td>0.978152</td>\n",
       "      <td>-1.416151</td>\n",
       "      <td>-0.627731</td>\n",
       "      <td>-0.040199</td>\n",
       "      <td>-1.554540</td>\n",
       "      <td>-0.495431</td>\n",
       "      <td>-0.491942</td>\n",
       "      <td>-0.492811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.047976</td>\n",
       "      <td>0.978152</td>\n",
       "      <td>-1.416151</td>\n",
       "      <td>-0.627731</td>\n",
       "      <td>-0.318474</td>\n",
       "      <td>-0.658049</td>\n",
       "      <td>-1.261117</td>\n",
       "      <td>-1.263332</td>\n",
       "      <td>-1.285335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.334497</td>\n",
       "      <td>-2.592365</td>\n",
       "      <td>-0.830504</td>\n",
       "      <td>-0.023048</td>\n",
       "      <td>4.899190</td>\n",
       "      <td>-0.658049</td>\n",
       "      <td>0.332578</td>\n",
       "      <td>0.216660</td>\n",
       "      <td>0.890505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      carat  cut_numeric  color_numeric  clarity_numeric     depth     table  \\\n",
       "0  3.267015     0.978152      -1.416151        -1.232414 -0.666318 -1.554540   \n",
       "1  2.596725     0.978152      -0.830504        -1.232414  0.168508 -0.658049   \n",
       "2 -0.608098     0.978152      -1.416151        -0.627731 -0.040199 -1.554540   \n",
       "3 -1.047976     0.978152      -1.416151        -0.627731 -0.318474 -0.658049   \n",
       "4  0.334497    -2.592365      -0.830504        -0.023048  4.899190 -0.658049   \n",
       "\n",
       "          x         y         z  \n",
       "0  2.620734  2.539800  2.446736  \n",
       "1  2.175567  2.234832  2.230593  \n",
       "2 -0.495431 -0.491942 -0.492811  \n",
       "3 -1.261117 -1.263332 -1.285335  \n",
       "4  0.332578  0.216660  0.890505  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamonds_predict.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(diamonds_train, diamonds_train_price, test_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut_numeric</th>\n",
       "      <th>color_numeric</th>\n",
       "      <th>clarity_numeric</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31052</th>\n",
       "      <td>-0.522664</td>\n",
       "      <td>0.086012</td>\n",
       "      <td>0.940769</td>\n",
       "      <td>-0.033632</td>\n",
       "      <td>-2.548318</td>\n",
       "      <td>2.033618</td>\n",
       "      <td>-0.240255</td>\n",
       "      <td>-0.306862</td>\n",
       "      <td>-0.546325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35986</th>\n",
       "      <td>0.873081</td>\n",
       "      <td>0.982588</td>\n",
       "      <td>0.940769</td>\n",
       "      <td>0.574315</td>\n",
       "      <td>-0.032574</td>\n",
       "      <td>-0.650176</td>\n",
       "      <td>0.972656</td>\n",
       "      <td>0.970219</td>\n",
       "      <td>0.961676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37647</th>\n",
       "      <td>0.217504</td>\n",
       "      <td>0.982588</td>\n",
       "      <td>-1.413663</td>\n",
       "      <td>-0.033632</td>\n",
       "      <td>0.177072</td>\n",
       "      <td>-1.992074</td>\n",
       "      <td>0.392956</td>\n",
       "      <td>0.431586</td>\n",
       "      <td>0.440218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21214</th>\n",
       "      <td>-0.818731</td>\n",
       "      <td>0.982588</td>\n",
       "      <td>0.352161</td>\n",
       "      <td>1.790210</td>\n",
       "      <td>0.037308</td>\n",
       "      <td>-0.202877</td>\n",
       "      <td>-0.873466</td>\n",
       "      <td>-0.819432</td>\n",
       "      <td>-0.828194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12204</th>\n",
       "      <td>-0.459221</td>\n",
       "      <td>0.982588</td>\n",
       "      <td>-0.236447</td>\n",
       "      <td>1.790210</td>\n",
       "      <td>-0.312101</td>\n",
       "      <td>-0.650176</td>\n",
       "      <td>-0.329440</td>\n",
       "      <td>-0.289487</td>\n",
       "      <td>-0.334923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          carat  cut_numeric  color_numeric  clarity_numeric     depth  \\\n",
       "31052 -0.522664     0.086012       0.940769        -0.033632 -2.548318   \n",
       "35986  0.873081     0.982588       0.940769         0.574315 -0.032574   \n",
       "37647  0.217504     0.982588      -1.413663        -0.033632  0.177072   \n",
       "21214 -0.818731     0.982588       0.352161         1.790210  0.037308   \n",
       "12204 -0.459221     0.982588      -0.236447         1.790210 -0.312101   \n",
       "\n",
       "          table         x         y         z  \n",
       "31052  2.033618 -0.240255 -0.306862 -0.546325  \n",
       "35986 -0.650176  0.972656  0.970219  0.961676  \n",
       "37647 -1.992074  0.392956  0.431586  0.440218  \n",
       "21214 -0.202877 -0.873466 -0.819432 -0.828194  \n",
       "12204 -0.650176 -0.329440 -0.289487 -0.334923  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22844</th>\n",
       "      <td>2943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4842</th>\n",
       "      <td>1875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39809</th>\n",
       "      <td>1939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18847</th>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20662</th>\n",
       "      <td>3462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       price\n",
       "22844   2943\n",
       "4842    1875\n",
       "39809   1939\n",
       "18847    473\n",
       "20662   3462"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark  Random forest\n",
    "https://spark.apache.org/docs/2.1.0/ml-classification-regression.html#random-forest-classifier<br>\n",
    "https://docs.microsoft.com/it-it/azure/hdinsight/spark/apache-spark-machine-learning-mllib-ipython<br>\n",
    "https://www.bmc.com/blogs/python-spark-machine-learning-classification/<br>\n",
    "https://databricks.com/spark/getting-started-with-apache-spark/machine-learning<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark --num-executors 5 --driver-memory 8g --executor-memory 8g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = ensemble.RandomForestClassifier(verbose=3) verbose para saber el tiempo de procesamiento\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_train['price']= diamonds_train_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = diamonds_train[['carat', 'cut_numeric','color_numeric','clarity_numeric','depth','table','x','y','z', 'price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+-----+\n",
      "|               carat|        cut_numeric|       color_numeric|     clarity_numeric|               depth|               table|                   x|                  y|                  z|price|\n",
      "+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+-----+\n",
      "|  -1.051354945258851|-0.8105638153544471| 0.35216095194517794|-0.03363208651838169|  0.7361262077726191| -0.6501764299800551| -1.2837156287583358|-1.2364383442799414| -1.180530943376829|  605|\n",
      "| -0.9667643697664698| 0.9825875572142716|   0.940769087416264| -0.6415793877253191|  0.5963626245318632| -1.0974755411742854|  -1.132101739428727|-1.0800610391334775|-1.0395962653514643|  565|\n",
      "| -0.8398785065278978|-0.8105638153544471|  1.5293772228873503| -0.6415793877253191| -1.0109185827368672|  2.0336182371853258| -0.9180586015516322|-0.8541827094774754|-0.9691289263387814|  720|\n",
      "| -0.8398785065278978|0.08601187092991222| -0.8250553189969944|  0.5743152146885556| 0.03730829156882455|  0.7811807258414829|  -0.900221673395208|-0.8628703375411676|-0.8704746517210261|  793|\n",
      "|  0.2175036871268695|-0.8105638153544471|  1.5293772228873503| -0.6415793877253191| -0.5217460413942091|   2.480917348379557|  0.3305263693980862|  0.344709963312076| 0.2710962402844311| 4381|\n",
      "|-0.41692562906599095| 0.9825875572142716|  1.5293772228873503| -0.6415793877253191|-0.03257350005155093| -0.6501764299800551| -0.3026845801549855|-0.2547363730826993|-0.2785490040144929| 2263|\n",
      "| -1.0090596575126605|-1.7071395016388065|   0.940769087416264| -0.6415793877253191|  0.9457715826337604| -1.5447746523685155| -1.2123679161326375| -1.149562063643017|-1.0818766687590735|  524|\n",
      "|  -1.051354945258851|0.08601187092991222|   0.940769087416264|-0.03363208651838169| -1.2205639575980036|   1.139020014796866| -1.1856125238980004|-1.1930002039614789|-1.2791852179945848|  844|\n",
      "| -0.5861067800507537|0.08601187092991222|-0.23644718352590824|-0.03363208651838169|  0.4565990412911022| -0.6501764299800551| -0.4810538617192304|-0.4979899588660868|-0.4335771498423943| 1577|\n",
      "| -0.8187308626548025|-0.8105638153544471|   0.940769087416264| -0.6415793877253191| 0.03730829156882455|   1.139020014796866| -0.8823847452387831|-0.8281198252863979|-0.8422877161159531|  705|\n",
      "|  -1.030207301385756| 0.9825875572142716|-0.23644718352590824| -1.2495266889322565|-0.10245529167193138| -0.8290960744577466| -1.2391233083672746|-1.1843125758977866|-1.1946244111793658|  429|\n",
      "|  -1.030207301385756|0.08601187092991222|-0.23644718352590824|  0.5743152146885556| 0.03730829156882455|  0.6917209036026356| -1.2569602365236987|-1.2016878320251718|-1.2087178789819022|  625|\n",
      "| -0.2054491903350376|-2.6037151879231657| -0.8250553189969944| -0.6415793877253191|  2.6229345815228657|-0.20287731878582488|-0.12431529859073975|-0.1591724643820834| 0.1583484978641389| 1865|\n",
      "|-0.09971097096956072|0.08601187092991222|-0.23644718352590824| -1.2495266889322565| -0.5217460413942091|  0.2444217924084053|  0.1521570878338413| 0.1796450301019208|0.10197462665399283| 2216|\n",
      "|  0.7250471400811576| 0.9825875572142716| -0.8250553189969944| -0.6415793877253191|-0.17233708329231182| -1.0974755411742854|  0.8834711422472475| 0.8833429032610051| 0.8489284201884275| 7079|\n",
      "|0.006027248395916...|-0.8105638153544471| 0.35216095194517794|  0.5743152146885556|   1.365062332356038| -0.6501764299800551| 0.12540169559920422|0.14489451784715116| 0.2992831758895041| 3460|\n",
      "|  2.5648921570404517| 0.9825875572142716| -0.8250553189969944| -0.6415793877253191|  0.2469536664299659| -0.6501764299800551|  2.1231376491187537| 2.0387974357320946| 2.1032470546141777|14725|\n",
      "|  0.8730806471928251| 0.9825875572142716| -0.8250553189969944|  1.1822625158954931| -0.3121006665330727| -0.6501764299800551|  0.9994111752640071| 1.0049696961526984| 0.9475826948061834| 8950|\n",
      "|  0.2175036871268695|-1.7071395016388065| -2.0022715899391668| -0.6415793877253191|  1.5747077072171745|  1.5863191259910958| 0.24134172861596376| 0.1970202862293052|0.41203091830979627| 2873|\n",
      "|  1.5075099633856857|-0.8105638153544471|   0.940769087416264|-0.03363208651838169| -1.1506821659776232|  0.2444217924084053|  1.5880298044260177| 1.5957284044837825|  1.398573664487352|14889|\n",
      "+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.registerTempTable(\"diamond_spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=['carat', 'cut_numeric','color_numeric','clarity_numeric','depth','table','x','y','z'],\n",
    "    outputCol=\"features\")\n",
    "output = assembler.transform(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+-----+--------------------+\n",
      "|               carat|        cut_numeric|       color_numeric|     clarity_numeric|               depth|               table|                   x|                  y|                  z|price|            features|\n",
      "+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+-----+--------------------+\n",
      "|  -1.051354945258851|-0.8105638153544471| 0.35216095194517794|-0.03363208651838169|  0.7361262077726191| -0.6501764299800551| -1.2837156287583358|-1.2364383442799414| -1.180530943376829|  605|[-1.0513549452588...|\n",
      "| -0.9667643697664698| 0.9825875572142716|   0.940769087416264| -0.6415793877253191|  0.5963626245318632| -1.0974755411742854|  -1.132101739428727|-1.0800610391334775|-1.0395962653514643|  565|[-0.9667643697664...|\n",
      "| -0.8398785065278978|-0.8105638153544471|  1.5293772228873503| -0.6415793877253191| -1.0109185827368672|  2.0336182371853258| -0.9180586015516322|-0.8541827094774754|-0.9691289263387814|  720|[-0.8398785065278...|\n",
      "| -0.8398785065278978|0.08601187092991222| -0.8250553189969944|  0.5743152146885556| 0.03730829156882455|  0.7811807258414829|  -0.900221673395208|-0.8628703375411676|-0.8704746517210261|  793|[-0.8398785065278...|\n",
      "|  0.2175036871268695|-0.8105638153544471|  1.5293772228873503| -0.6415793877253191| -0.5217460413942091|   2.480917348379557|  0.3305263693980862|  0.344709963312076| 0.2710962402844311| 4381|[0.21750368712686...|\n",
      "|-0.41692562906599095| 0.9825875572142716|  1.5293772228873503| -0.6415793877253191|-0.03257350005155093| -0.6501764299800551| -0.3026845801549855|-0.2547363730826993|-0.2785490040144929| 2263|[-0.4169256290659...|\n",
      "| -1.0090596575126605|-1.7071395016388065|   0.940769087416264| -0.6415793877253191|  0.9457715826337604| -1.5447746523685155| -1.2123679161326375| -1.149562063643017|-1.0818766687590735|  524|[-1.0090596575126...|\n",
      "|  -1.051354945258851|0.08601187092991222|   0.940769087416264|-0.03363208651838169| -1.2205639575980036|   1.139020014796866| -1.1856125238980004|-1.1930002039614789|-1.2791852179945848|  844|[-1.0513549452588...|\n",
      "| -0.5861067800507537|0.08601187092991222|-0.23644718352590824|-0.03363208651838169|  0.4565990412911022| -0.6501764299800551| -0.4810538617192304|-0.4979899588660868|-0.4335771498423943| 1577|[-0.5861067800507...|\n",
      "| -0.8187308626548025|-0.8105638153544471|   0.940769087416264| -0.6415793877253191| 0.03730829156882455|   1.139020014796866| -0.8823847452387831|-0.8281198252863979|-0.8422877161159531|  705|[-0.8187308626548...|\n",
      "|  -1.030207301385756| 0.9825875572142716|-0.23644718352590824| -1.2495266889322565|-0.10245529167193138| -0.8290960744577466| -1.2391233083672746|-1.1843125758977866|-1.1946244111793658|  429|[-1.0302073013857...|\n",
      "|  -1.030207301385756|0.08601187092991222|-0.23644718352590824|  0.5743152146885556| 0.03730829156882455|  0.6917209036026356| -1.2569602365236987|-1.2016878320251718|-1.2087178789819022|  625|[-1.0302073013857...|\n",
      "| -0.2054491903350376|-2.6037151879231657| -0.8250553189969944| -0.6415793877253191|  2.6229345815228657|-0.20287731878582488|-0.12431529859073975|-0.1591724643820834| 0.1583484978641389| 1865|[-0.2054491903350...|\n",
      "|-0.09971097096956072|0.08601187092991222|-0.23644718352590824| -1.2495266889322565| -0.5217460413942091|  0.2444217924084053|  0.1521570878338413| 0.1796450301019208|0.10197462665399283| 2216|[-0.0997109709695...|\n",
      "|  0.7250471400811576| 0.9825875572142716| -0.8250553189969944| -0.6415793877253191|-0.17233708329231182| -1.0974755411742854|  0.8834711422472475| 0.8833429032610051| 0.8489284201884275| 7079|[0.72504714008115...|\n",
      "|0.006027248395916...|-0.8105638153544471| 0.35216095194517794|  0.5743152146885556|   1.365062332356038| -0.6501764299800551| 0.12540169559920422|0.14489451784715116| 0.2992831758895041| 3460|[0.00602724839591...|\n",
      "|  2.5648921570404517| 0.9825875572142716| -0.8250553189969944| -0.6415793877253191|  0.2469536664299659| -0.6501764299800551|  2.1231376491187537| 2.0387974357320946| 2.1032470546141777|14725|[2.56489215704045...|\n",
      "|  0.8730806471928251| 0.9825875572142716| -0.8250553189969944|  1.1822625158954931| -0.3121006665330727| -0.6501764299800551|  0.9994111752640071| 1.0049696961526984| 0.9475826948061834| 8950|[0.87308064719282...|\n",
      "|  0.2175036871268695|-1.7071395016388065| -2.0022715899391668| -0.6415793877253191|  1.5747077072171745|  1.5863191259910958| 0.24134172861596376| 0.1970202862293052|0.41203091830979627| 2873|[0.21750368712686...|\n",
      "|  1.5075099633856857|-0.8105638153544471|   0.940769087416264|-0.03363208651838169| -1.1506821659776232|  0.2444217924084053|  1.5880298044260177| 1.5957284044837825|  1.398573664487352|14889|[1.50750996338568...|\n",
      "+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.registerTempTable(\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val Array(trainingData, testData) = df_spark.randomSplit(Array(0.8, 0.2)).setImpurity(\"gini\").setMaxDepth(3).setNumTrees(20).setFeatureSubsetStrategy(\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_spark = spark.sql(\"\"\"\n",
    "SELECT features,price\n",
    "FROM output\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|price|\n",
      "+--------------------+-----+\n",
      "|[-1.0513549452588...|  605|\n",
      "|[-0.9667643697664...|  565|\n",
      "|[-0.8398785065278...|  720|\n",
      "|[-0.8398785065278...|  793|\n",
      "|[0.21750368712686...| 4381|\n",
      "|[-0.4169256290659...| 2263|\n",
      "|[-1.0090596575126...|  524|\n",
      "|[-1.0513549452588...|  844|\n",
      "|[-0.5861067800507...| 1577|\n",
      "|[-0.8187308626548...|  705|\n",
      "|[-1.0302073013857...|  429|\n",
      "|[-1.0302073013857...|  625|\n",
      "|[-0.2054491903350...| 1865|\n",
      "|[-0.0997109709695...| 2216|\n",
      "|[0.72504714008115...| 7079|\n",
      "|[0.00602724839591...| 3460|\n",
      "|[2.56489215704045...|14725|\n",
      "|[0.87308064719282...| 8950|\n",
      "|[0.21750368712686...| 2873|\n",
      "|[1.50750996338568...|14889|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_spark = data_spark.withColumnRenamed(\"price\",\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[-1.0513549452588...|  605|\n",
      "|[-0.9667643697664...|  565|\n",
      "|[-0.8398785065278...|  720|\n",
      "|[-0.8398785065278...|  793|\n",
      "|[0.21750368712686...| 4381|\n",
      "|[-0.4169256290659...| 2263|\n",
      "|[-1.0090596575126...|  524|\n",
      "|[-1.0513549452588...|  844|\n",
      "|[-0.5861067800507...| 1577|\n",
      "|[-0.8187308626548...|  705|\n",
      "|[-1.0302073013857...|  429|\n",
      "|[-1.0302073013857...|  625|\n",
      "|[-0.2054491903350...| 1865|\n",
      "|[-0.0997109709695...| 2216|\n",
      "|[0.72504714008115...| 7079|\n",
      "|[0.00602724839591...| 3460|\n",
      "|[2.56489215704045...|14725|\n",
      "|[0.87308064719282...| 8950|\n",
      "|[0.21750368712686...| 2873|\n",
      "|[1.50750996338568...|14889|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_y_spark = spark.sql(\"\"\"\\nSELECT price\\nFROM trainingData\"\"\")'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''train_y_spark = spark.sql(\"\"\"\n",
    "SELECT price\n",
    "FROM trainingData\"\"\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testData.registerTempTable(\"testData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'testData = spark.sql(\"\"\"\\nSELECT features, price\\nFROM testData\"\"\")\\ntestData.show()'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''testData = spark.sql(\"\"\"\n",
    "SELECT features, price\n",
    "FROM testData\"\"\")\n",
    "testData.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data_spark)\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\").fit(data_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data_spark.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"label\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o116.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 14.0 failed 1 times, most recent failure: Lost task 4.0 in stage 14.0 (TID 66, LISCIA-OMEN-by-HP-Laptop.lan, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$3419/0x0000000101505840.apply(Unknown Source)\n\tat scala.Array$.tabulate(Array.scala:334)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$3392/0x00000001014be040.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:837)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:837)\n\tat org.apache.spark.rdd.RDD$$Lambda$1926/0x0000000100f0f840.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1894/0x0000000100eeec40.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:737)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:736)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:302)\n\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:161)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:138)\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:46)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:150)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$3419/0x0000000101505840.apply(Unknown Source)\n\tat scala.Array$.tabulate(Array.scala:334)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$3392/0x00000001014be040.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:837)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:837)\n\tat org.apache.spark.rdd.RDD$$Lambda$1926/0x0000000100f0f840.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1894/0x0000000100eeec40.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-ab0cb0b1820a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train model.  This also runs the indexers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainingData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/spark/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[1;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o116.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 14.0 failed 1 times, most recent failure: Lost task 4.0 in stage 14.0 (TID 66, LISCIA-OMEN-by-HP-Laptop.lan, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$3419/0x0000000101505840.apply(Unknown Source)\n\tat scala.Array$.tabulate(Array.scala:334)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$3392/0x00000001014be040.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:837)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:837)\n\tat org.apache.spark.rdd.RDD$$Lambda$1926/0x0000000100f0f840.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1894/0x0000000100eeec40.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:737)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:736)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:302)\n\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:161)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:138)\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:46)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:150)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$3419/0x0000000101505840.apply(Unknown Source)\n\tat scala.Array$.tabulate(Array.scala:334)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$3392/0x00000001014be040.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:837)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:837)\n\tat org.apache.spark.rdd.RDD$$Lambda$1926/0x0000000100f0f840.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1894/0x0000000100eeec40.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 51072)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/must4in3/anaconda3/lib/python3.7/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/must4in3/anaconda3/lib/python3.7/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/must4in3/anaconda3/lib/python3.7/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/must4in3/anaconda3/lib/python3.7/socketserver.py\", line 720, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/must4in3/spark/python/pyspark/accumulators.py\", line 268, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/must4in3/spark/python/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/home/must4in3/spark/python/pyspark/accumulators.py\", line 245, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/home/must4in3/spark/python/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions.\n",
    "predictions_rf = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select example rows to display.\n",
    "predictions_rf.select(\"predictedLabel\", \"label\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = predictions_rf.select(\"predictedLabel\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"predictedLabel\", \"label\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "rfModel = model.stages[2]\n",
    "print(rfModel)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_spark = data_spark.withColumnRenamed(\"price\",\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelA = lr.fit(data_spark, {lr.regParam:0.0})\n",
    "modelB = lr.fit(data_spark, {lr.regParam:100.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsA = modelA.transform(data_spark)\n",
    "predictionsA.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pdf = predictionsA.select(\"*\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Errore quadratico medio:% .2f\" % mean_squared_error ( result_pdf['label'], result_pdf['prediction']))\n",
    "print( 'R²:% .2f' % r2_score ( result_pdf['label'], result_pdf['prediction']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest without spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.get_dummies(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, max_depth=40, \n",
    "                                 n_jobs=-1, verbose=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y_rf = pd.DataFrame(y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(class_weight='balanced', \n",
    "                                 solver ='saga' , n_jobs=-1, verbose=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ylogistic = pd.DataFrame(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ylogistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_logistic = y_test.copy()\n",
    "y_test_logistic['y']= y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_logistic['Price-y'] = abs(y_test_logistic['price']- y_test_logistic['y'])\n",
    "y_test_logistic.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "print( \"Errore quadratico medio:% .2f\" % mean_squared_error ( y_test_logistic['price'], y_test_logistic['y']))\n",
    "print( 'R²:% .2f' % r2_score ( y_test_logistic['price'], y_test_logistic['y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
